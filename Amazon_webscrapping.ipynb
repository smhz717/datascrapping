{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "354825be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import smtplib\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c82d7ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   Funny Got Data MIS Data Systems Business Analyst T-Shirt\n",
      "                   \n",
      "\n",
      "\n",
      "                    Fit Type:\n",
      "                   \n",
      "\n",
      "                    Men\n",
      "                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                             Men\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                             Women\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                             Youth\n",
      "                            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                       4.8 out of 5 stars\n",
      "                      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    15 ratings\n",
      "                   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# connecting to website \n",
    "URL = 'https://www.amazon.com/Funny-Data-Systems-Business-Analyst/dp/B07FNW9FGJ/ref=sr_1_3?dchild=1&keywords=data%2Banalyst%2Btshirt&qid=1626655184&sr=8'\n",
    "\n",
    "# get the user-agent from httpbin.org/get, the rest just copy paste\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\", \n",
    "     \"Accept-Encoding\":\"gzip, deflate\", \n",
    "           \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "           \"DNT\":\"1\",\"Connection\":\"close\", \n",
    "           \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "\n",
    "# using requests and beautifulsoup\n",
    "page = requests.get(URL, headers=headers)\n",
    "# get html of the whole page\n",
    "soup1 = BeautifulSoup(page.content, 'html.parser')\n",
    "# get clean html\n",
    "soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "#  get the title, the id you can find from inspecting the page on google chrome\n",
    "title=soup2.find(id='productTitle').get_text()\n",
    "fit =soup2.find(id='variation_fit_type').get_text()\n",
    "review=soup2.find(id='averageCustomerReviews').get_text()\n",
    "\n",
    "print(title, fit, review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6bc73489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men', 'Women', 'Youth']\n"
     ]
    }
   ],
   "source": [
    "#cleaning the junk \n",
    "fit1=fit.strip().replace('\\n',\"\")\n",
    "fit_f= fit1.split()\n",
    "fit_final=fit_f[3:]\n",
    "print(fit_final)\n",
    "\n",
    "title=title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "083b14cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "review = review.strip()\n",
    "review1=review.split()\n",
    "reviews_rating= review1[0]\n",
    "no_of_reviews = review1[-2]\n",
    "print(reviews_rating)\n",
    "print(no_of_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding date when the record was added\n",
    "date = datetime.date.today()\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bec7f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the data into a csv file\n",
    "header = ['Title', 'Rating','Total Ratings', 'Date']\n",
    "data= [title , reviews_rating , no_of_reviews, date]\n",
    "#type(data)\n",
    "with open('Amazonwebscrapping.csv','w',newline='',encoding='UTF8') as f:  #w here means write\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1346d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Rating  Total Ratings  \\\n",
      "0  Funny Got Data MIS Data Systems Business Analy...     4.8             15   \n",
      "1  Funny Got Data MIS Data Systems Business Analy...     4.8             15   \n",
      "2  Funny Got Data MIS Data Systems Business Analy...     4.8             15   \n",
      "3  Funny Got Data MIS Data Systems Business Analy...     4.8             15   \n",
      "4  Funny Got Data MIS Data Systems Business Analy...     4.8             15   \n",
      "5  Funny Got Data MIS Data Systems Business Analy...     4.8             15   \n",
      "\n",
      "         Date  \n",
      "0  2022-08-05  \n",
      "1  2022-08-05  \n",
      "2  2022-08-05  \n",
      "3  2022-08-05  \n",
      "4  2022-08-05  \n",
      "5  2022-08-05  \n"
     ]
    }
   ],
   "source": [
    "# this code is so that i dont have to open csv everytime anything is updated in data\n",
    "df= pd.read_csv(r'C:\\Users\\smhz_\\PycharmProjects\\Amazonwebscrapping.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2f6b00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now adding more rows in the data\n",
    "with open('Amazonwebscrapping.csv','a+',newline='',encoding='UTF8') as f:  #a+ here means appending\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "89eb1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all of the above code into one function\n",
    "\n",
    "\n",
    "def check_price():\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36\", \n",
    "     \"Accept-Encoding\":\"gzip, deflate\", \n",
    "           \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "           \"DNT\":\"1\",\"Connection\":\"close\", \n",
    "           \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "\n",
    "    # using requests and beautifulsoup\n",
    "    page = requests.get(URL, headers=headers)\n",
    "    # get html of the whole page\n",
    "    soup1 = BeautifulSoup(page.content, 'html.parser')\n",
    "    # get clean html\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "    #  get the title, the id you can find from inspecting the page on google chrome\n",
    "    title=soup2.find(id='productTitle').get_text()\n",
    "    fit =soup2.find(id='variation_fit_type').get_text()\n",
    "    review=soup2.find(id='averageCustomerReviews').get_text()   \n",
    "    title=title.strip()\n",
    "    review = review.strip()\n",
    "    review1=review.split()\n",
    "    reviews_rating= review1[0]\n",
    "    no_of_reviews = review1[-2]\n",
    "    date = datetime.date.today()\n",
    "    #writing the data into a csv file\n",
    "    header = ['Title', 'Rating','Total Ratings', 'Date']\n",
    "    data= [title , reviews_rating , no_of_reviews, date]\n",
    "    with open('Amazonwebscrapping.csv','a+',newline='',encoding='UTF8') as f:  #a+ here means appending\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a458277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs check_price after a set time and inputs data into your CSV\n",
    "while(True):\n",
    "    check_price()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafe86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
